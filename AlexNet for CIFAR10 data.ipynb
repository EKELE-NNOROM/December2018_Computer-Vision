{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout,Activation, Flatten\n",
    "from keras.layers import MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Display our data shape/dimensions\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Now we one hot encode outputs\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 256)       614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 10, 10, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 9, 9, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 9, 9, 1024)        9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 9, 9, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 9, 9, 1024)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              50334720  \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 3072)              12288     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              12587008  \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 78,990,642\n",
      "Trainable params: 78,970,462\n",
      "Non-trainable params: 20,180\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#import keras\n",
    "l2_reg = 0\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Conv Layer\n",
    "model.add(Conv2D(96, (11, 11), input_shape=x_train.shape[1:], \n",
    "                 padding='same',kernel_regularizer=l2(l2_reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 2nd Conv Layer\n",
    "model.add(Conv2D(256, (5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 3rd Conv Layer\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Conv2D(512, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# 4th Conv Layer \n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 5th Conv Layer \n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Conv2D(1024, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# 1st FC Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3072))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 2nd FC Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 3rd FC Layer\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"CIFAR10_AlexNet_10-{epoch:02d}-{val_acc:.2f}.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "callbacks = [earlystop, checkpoint]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 400s 8ms/step - loss: 1.5307 - acc: 0.4543 - val_loss: 1.3645 - val_acc: 0.5261\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.36454, saving model to CIFAR10_AlexNet_10-01-0.53.h5\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 397s 8ms/step - loss: 1.1510 - acc: 0.6000 - val_loss: 1.0610 - val_acc: 0.6389\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.36454 to 1.06101, saving model to CIFAR10_AlexNet_10-02-0.64.h5\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 398s 8ms/step - loss: 0.9489 - acc: 0.6759 - val_loss: 0.9764 - val_acc: 0.6717\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.06101 to 0.97644, saving model to CIFAR10_AlexNet_10-03-0.67.h5\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 398s 8ms/step - loss: 0.8042 - acc: 0.7295 - val_loss: 0.7810 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.97644 to 0.78104, saving model to CIFAR10_AlexNet_10-04-0.74.h5\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 398s 8ms/step - loss: 0.6842 - acc: 0.7724 - val_loss: 0.7253 - val_acc: 0.7579\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.78104 to 0.72531, saving model to CIFAR10_AlexNet_10-05-0.76.h5\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 399s 8ms/step - loss: 0.5733 - acc: 0.8094 - val_loss: 0.7901 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.72531\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 399s 8ms/step - loss: 0.4714 - acc: 0.8457 - val_loss: 0.7706 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.72531\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 399s 8ms/step - loss: 0.3794 - acc: 0.8788 - val_loss: 0.7270 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.72531\n",
      "Epoch 00008: early stopping\n",
      "10000/10000 [==============================] - 7s 658us/step\n",
      "Test loss: 0.7269818831205368\n",
      "Test accuracy: 0.7637\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "# Save model\n",
    "model.save(\"TrainedModel/cifar10_AlexNet.h5\")\n",
    "\n",
    "# Evaluate model\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUDEEP",
   "language": "python",
   "name": "gpudeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
